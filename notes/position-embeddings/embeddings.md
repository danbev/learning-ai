## Embeddings
When working with natural language models the input is text but neural networks
operate on matrices/tensors of numbers, not text. Embedding vectors are a way
to turn text into vectors with the elements of the vector being numbers so that
text can be used with neural networks.

How this text to vectors of numbers is done can differ, and one example is doing
[one-hot-encoding](../one-hot-encoding.md). Another option is using a count
based approach. And we also have the option to use embeddings which is what this
document will address.

It's a way of representing data like strings, music, video as points in an
n-dimension space. Doing this can allow similar data points to cluster together.

One thing to note is that embeddings are generated by training a model and the
embeddings are influenced by the training data. An example of this is that the
word cold might be close to the word winter in the embedding space because they
often appear together in the training data for a general model. But if the model
was trained on medical data then the same word cold might be closer to the word
flu or pneumonia.

Word to vector (Word2Vec) was invented by Google in 2013 which takes as input
a word and outputs an n-dimensional coordinate, a vector. So, simliar words
would be closer to each other. Think of this as the tip of the vectors are in
close proximity to each other if the words are simliar.

For songs similar sounding songs would be nearby each other. And for images,
simliar looking images would be closer to each other. This could be anything
really so we don't have to think just in terms of words.

How is this useful?  
Well, we can look at vectors/points close to get similar words, songs, movies
etc. This is called nearest neighbor search.

Embedding also allows us to compute similarity scores between these points
allowing us to ask how similar is this song to another song. We can use the
Euclidean distance, the dot product, cosine distance, etc to calculate this
distance.

The learning stage is what positions/moves these vectors close to each other.

We can use a one dimensional vector to represent each word, for example:
```
Why He She They Gras Tree
1   3   4   5    10   11

        She                  Tree
Why   He   they           Gras
|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-
1  2  3  4  5  6  7  8  0 10 11 12 13 14 
```
This way we can calculate how similar to word are using:
```
Why and Gras:
10 - 1 = 9

He and she:
4 - 3 = 1
```
We can also define the encoding should have two values per vector making it
2 dimensional. So instead of each word being a single value it each one will
be a vector. We can use more dimensions as well and they can be.


### Universal sentence encoder
In this case entire sentences can be transformed into vectors which enables us
to do the same with sentences that we could with single words above.

### llama.cpp embeddings example
The `llama-embeddings` example in llama.cpp is an example of how we can generate
embeddings for a given input. As an overview of this process

We pass in a prompt to the LLM which will tokenize it and then lookup the
token embeddings for each token in the prompt. The embeddings are not context
aware, meaning that the embeddings are the same for a given token regardless of
the context in which it appears. This is then passed to the LLM and goes through
the forward pass of the model. The final layer of the of the forward pass this
is a matrix with one row for each token in the token sequence, and the
dimensions is the embedding dimension. Now, what should be returned here is
as single token embedding and this is where pooling (reduction/compression) comes
into play. The pooling decides how this embedding to be returned should look
like. For example lets say our final layer looks like this:
```
token 0   [0.1, 0.2, -0.3, 0.4]
token 1   [0.5, 0.1, 0.2, -0.1]
token 2   [-0.2, 0.3, 0.4, 0.2]
```
In `llama-embedings` there are the following pooling options:
```console
--pooling {none,mean,cls,last,rank}     pooling type for embeddings, use model default if unspecified
                                        (env: LLAMA_ARG_POOLING)
```
Using `last` would simply return the last token, token2:
```
last_pooling = [-0.2, 0.3, 0.4, 0.2]
```
`mean` pooling would return the mean of each dimension:
```
mean_pooling = [0.13, 0.20, 0.10, 0.17]
```
`max` pooling would return the maximum value of each dimension:
```
max_pooling = [0.5, 0.3, 0.4, 0.4]
```

`cls` pooling would return the first token which is the case of CLS is a
special token that is inserted and it is not a token from the input prompt:
```
CLS token [0.8, -0.1, 0.3, 0.2
token 0   [0.1, 0.2, -0.3, 0.4]
token 1   [0.5, 0.1, 0.2, -0.1]
token 2   [-0.2, 0.3, 0.4, 0.2]

cls_pooling = [0.8, -0.1, 0.3, 0.2]
```
The model must have been trained with this special classification token that is
meant to capture the overall meaning of the input sequence.

There is also `rank` which I need to look into more closely: TODO: rank pooling

And then normalization will happen on this vector that the pooling produces.
```console
--embd-normalize N                      normalisation for embendings (default: 2) (-1=none, 0=max absolute
                                        int16, 1=taxicab, 2=euclidean, >2=p-norm)
```
So `-1/none` will just return the values as they that the pooling produced.
`0` will normalize the values to the max absolute int16 value. `1` will use
taxicab/manhattan/L1 normalization, `2` will use euclidean/L2 normalization and
values greater than 2 will use p-norm normalization.

The example can be used as follows:
```console
$ gdb --args ./llama-embedding -m models/llama-2-7b-chat.Q4_K_M.gguf --pooling mean  -p "What is LoRA?"
```
Now, recall that first the prompt is split into tokens, which each have an id
from the model vocabulary.

This example will set `params.embeddings = true`: 
```c++
    params.embedding = true;
```
First we tokenize the prompt like we mentioned above.
```c++
        auto inp = ::llama_tokenize(ctx, prompt, true, false);
```
```console
(gdb) p inp
$2 = std::vector of length 6, capacity 15 = {1, 1724, 338, 4309, 4717, 29973}
(gdb) p model.vocab.id_to_token[1]
$6 = {text = "<s>", score = 0, attr = LLAMA_TOKEN_ATTR_CONTROL}
(gdb) p model.vocab.id_to_token[1724]
$7 = {text = "▁What", score = -1465, attr = LLAMA_TOKEN_ATTR_NORMAL}
(gdb) p model.vocab.id_to_token[338]
$8 = {text = "▁is", score = -79, attr = LLAMA_TOKEN_ATTR_NORMAL}
```
So we have tokens for the prompt we passed in.
For the model in this example it has an embedding size of 4096 and we will
create a vector large enough to hold an embedding:
```c++
    std::vector<float> embeddings(n_prompts * n_embd, 0);
```
A this point all values in the dimensions are zero.
Next we create a pointer to the above vectors data:
```c++
    float * emb = embeddings.data();

    // final batch
    float * out = emb + p * n_embd;
    batch_decode(ctx, batch, out, s, n_embd, params.embd_normalize);
```
The batch looks like this:
```console
(gdb) p batch
$38 = {n_tokens = 6, token = 0x555555b2e570, embd = 0x0, pos = 0x555555b30580, n_seq_id = 0x555555b32590, 
  seq_id = 0x555555b345a0, logits = 0x555555ed1510 "\001\001\001\001\001\001", all_pos_0 = 0, all_pos_1 = 0, 
  all_seq_id = 0}
```
We  will call `llama_decode` just like we would for a normal decoding:
```c++
    if (llama_decode(ctx, batch) < 0) {
        fprintf(stderr, "%s : failed to decode\n", __func__);
    }
```
In `llama_decode` we can find the following:
```c++
    // this indicates we are doing pooled embedding, so we ignore batch.logits and output all tokens
    const bool embd_pooled = cparams.embeddings && cparams.pooling_type != LLAMA_POOLING_TYPE_NONE;

        } else if (cparams.embeddings) {
            res = nullptr; // do not extract logits for embedding case
            embd = gf->nodes[gf->n_nodes - 1];
            if (strcmp(embd->name, "result_embd_pooled") != 0) {
                embd = gf->nodes[gf->n_nodes - 2];
            }
            GGML_ASSERT(strcmp(embd->name, "result_embd_pooled") == 0 && "missing embeddings tensor");
```
And the `embd` tensor will look like this:
```console
$56 = {type = GGML_TYPE_F32,
backend = GGML_BACKEND_TYPE_CPU,
buffer = 0x0,
ne = {4096, 6, 1, 1},
nb = {4, 16384, 98304, 98304},
op = GGML_OP_MUL_MAT, op_params = {0 <repeats 16 times>}, flags = 0, grad = 0x0, 
src = {0x7ffe8402fac0, 0x7ffe8402f7e0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, 
view_offs = 0, data = 0x0,
name = "result_embd_pooled", '\000' <repeats 45 times>, extra = 0x0}
```
So we can see that this has 6 rows and 4096 columns.
A little later in `llama_decode_internal` we have:
```c++
        llama_set_inputs(lctx, u_batch);
```
In our case we are using mean pooling. And just to be clear on what this, it is
called pooling because we are reducing/compressing a sequence of token
embeddings into a single embedding. This can be done in many different ways. In
mean pooling we take the average of each embedding dimension across all tokens
in a sequence.
```c++
    if (cparams.embeddings && cparams.pooling_type == LLAMA_POOLING_TYPE_MEAN) {
        const int64_t n_tokens = batch.n_tokens;

        GGML_ASSERT(lctx.inp_mean);
        GGML_ASSERT(ggml_backend_buffer_is_host(lctx.inp_mean->buffer));

        float * data = (float *) lctx.inp_mean->data;
        memset(lctx.inp_mean->data, 0, n_tokens * n_tokens * ggml_element_size(lctx.inp_mean));

        std::vector<uint64_t> sum(n_tokens, 0);
        for (int i = 0; i < n_tokens; ++i) {
            const llama_seq_id seq_id = batch.seq_id[i][0];

            GGML_ASSERT(seq_id < n_tokens && "seq_id cannot be larger than n_tokens with pooling_type == MEAN");

            sum[seq_id] += 1;
        }

        std::vector<float> div(n_tokens, 0.0f);
        for (int i = 0; i < n_tokens; ++i) {
            const uint64_t s = sum[i];
            if (s > 0) {
                div[i] = 1.0f/float(s);
            }
        }

        for (int i = 0; i < n_tokens; ++i) {
            const llama_seq_id seq_id = batch.seq_id[i][0];
            data[seq_id*n_tokens + i] = div[seq_id];
        }
    }
```
The above is setting up the a tensor, `inp_mean` to hold the matrix operation
that performs mean calculation.This tensor is created by the function
`llm.append_pooling` which is called by `llama_build_graph`.
```c++
    // add on pooling layer
    if (lctx.cparams.embeddings) {
        result = llm.append_pooling(result);
    }
```
And in `append_pooling` we find:
```c++

        switch (pooling_type) {
            case LLAMA_POOLING_TYPE_MEAN:
                {
                    struct ggml_tensor * inp_mean = build_inp_mean();
                    cur = ggml_mul_mat(ctx0, ggml_cont(ctx0, ggml_transpose(ctx0, inp)), inp_mean);
                } break;
```
And the `build_inp_mean` function looks like this:
```c++
    struct ggml_tensor * build_inp_mean() {
        lctx.inp_mean = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, n_tokens, n_tokens);
        cb(lctx.inp_mean, "inp_mean", -1);
        ggml_set_input(lctx.inp_mean);
        return lctx.inp_mean;
    }
```
So in this case we will have a 6x6 matrix to hold the mean values which indeed
is the case:
```console
(gdb) p *lctx.inp_mean 
$62 = {type = GGML_TYPE_F32,
backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555aab7c0,
ne = {6, 6, 1, 1}, 
nb = {4, 24, 144, 144},
op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 1, grad = 0x0, src = {
0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffe3a230020, 
name = "inp_mean", '\000' <repeats 55 times>, extra = 0x0}
```

If we look back as how this tensor is populated we can see that is first figures
out how many tokens each sequence has. Potentially each token could belong to a
different sequence to at most 6 sums are stored in the `sum` vector.
```c++
        std::vector<uint64_t> sum(n_tokens, 0);
        for (int i = 0; i < n_tokens; ++i) {
            const llama_seq_id seq_id = batch.seq_id[i][0];

            GGML_ASSERT(seq_id < n_tokens && "seq_id cannot be larger than n_tokens with pooling_type == MEAN");

            sum[seq_id] += 1;
        }
```
```console
(gdb) p sum
$65 = std::vector of length 6, capacity 6 = {6, 0, 0, 0, 0, 0}
```
And then we will calculate the divisors for each sequence:
```c++
        std::vector<float> div(n_tokens, 0.0f);
        for (int i = 0; i < n_tokens; ++i) {
            const uint64_t s = sum[i];
            if (s > 0) {
                div[i] = 1.0f/float(s);
            }
        }
```
Instead of doing a sum followed by a division we can do the division this is
setting up the tensor `inp_mean` to hold reciprocol of the sum for each
sequence. So instead of sum and divide this can be done by a single matrix
multiplication:
```c++
     struct ggml_tensor * inp_mean = build_inp_mean();
     cur = ggml_mul_mat(ctx0, ggml_cont(ctx0, ggml_transpose(ctx0, inp)), inp_mean);
```

```console
(gdb) p div
$69 = std::vector of length 6, capacity 6 = {0.166666672, 0, 0, 0, 0, 0}
```
```c++
        float * data = (float *) lctx.inp_mean->data;

        for (int i = 0; i < n_tokens; ++i) {
            const llama_seq_id seq_id = batch.seq_id[i][0];
            data[seq_id*n_tokens + i] = div[seq_id];
        }
```
```console
(gdb) p data[0]
$78 = 0.166666672
(gdb) p data[1]
$79 = 0.166666672
(gdb) p data[2]
$80 = 0.166666672
(gdb) p data[3]
$81 = 0.166666672
(gdb) p data[4]
$82 = 0.166666672
(gdb) p data[5]
$83 = 0.166666672
(gdb) p data[6]
$84 = 0
```
Notice that only the first 6 elements are set to 0.166666672. The rest are zero.
and this is a 6x6 matrix so we have 36 elements in total.
And this matrix is then used in a matrix multiplation:

```c++
        switch (pooling_type) {
            case LLAMA_POOLING_TYPE_MEAN:
                {
                    struct ggml_tensor * inp_mean = build_inp_mean();
                    cur = ggml_mul_mat(ctx0, ggml_cont(ctx0, ggml_transpose(ctx0, inp)), inp_mean);
                } break;
```

In `llama_decode_internal` we also have:
```c++
                case LLAMA_POOLING_TYPE_MEAN:
                case LLAMA_POOLING_TYPE_CLS:
                case LLAMA_POOLING_TYPE_LAST:
                    {
                        // extract sequence embeddings
                        auto & embd_seq_out = lctx.embd_seq;
                        embd_seq_out.clear();

                        for (uint32_t i = 0; i < n_tokens; i++) {
                            const llama_seq_id seq_id = u_batch.seq_id[i][0];
                            if (embd_seq_out.find(seq_id) != embd_seq_out.end()) {
                                continue;
                            }
                            embd_seq_out[seq_id].resize(n_embd);
                            ggml_backend_tensor_get_async(backend_embd,
                                embd,
                                embd_seq_out[seq_id].data(),
                                (n_embd*seq_id)*sizeof(float), n_embd*sizeof(float));
                        }
                    } break;
```

And then in the for loop for embeddings (in embeddings.cpp):
```c++
    for (int i = 0; i < batch.n_tokens; i++) {
        if (!batch.logits[i]) {
            continue;
        }

        // try to get sequence embeddings - supported only when pooling_type is not NONE
        const float * embd = llama_get_embeddings_seq(ctx, batch.seq_id[i][0]);
        GGML_ASSERT(embd != NULL && "failed to get sequence embeddings");

        float * out = output + batch.seq_id[i][0] * n_embd;
        llama_embd_normalize(embd, out, n_embd, embd_norm);
    }
```
We retrieve the embeddings for the sequence id.


In common.h we have the parameter for embedding normalization:
```c++
    int32_t embd_normalize = 2;     // normalisation for embendings (-1=none, 0=max absolute int16, 1=taxicab, 2=euclidean, >2=p-norm)
```
But the taxicab normalization is not implementes or perhaps it has been removed:
```c++
void llama_embd_normalize(const float * inp, float * out, int n, int embd_norm) {
    double sum = 0.0;

    switch (embd_norm) {
        case -1: // no normalisation
            sum = 1.0;
            break;
        case 0: // max absolute
            for (int i = 0; i < n; i++) {
                if (sum < std::abs(inp[i])) sum = std::abs(inp[i]);
            }
            sum /= 32760.0; // make an int16 range
            break;
        case 2: // euclidean
            for (int i = 0; i < n; i++) {
                sum += inp[i] * inp[i];
            }
            sum = std::sqrt(sum);
            break;
        default: // p-norm (euclidean is p-norm p=2)
            for (int i = 0; i < n; i++) {
                sum += std::pow(std::abs(inp[i]), embd_norm);
            }
            sum = std::pow(sum, 1.0 / embd_norm);
            break;
    }

    const float norm = sum > 0.0 ? 1.0 / sum : 0.0f;

    for (int i = 0; i < n; i++) {
        out[i] = inp[i] * norm;
    }
}
```

There are also other options for the type of pooling which are:
```console
$ ./llama-embedding --help | grep pooling
         --pooling {none,mean,cls,last}
                                  pooling type for embeddings, use model default if unspecified
```
Let take a look at using `last` pooling. First we can see that in
`append_pooling` there is a case clause for both `LLAMA_POOLING_TYPE_CLS` and
`LLAMA_POOLING_TYPE_LAST`
```c
            case LLAMA_POOLING_TYPE_CLS:
            case LLAMA_POOLING_TYPE_LAST:
                {
                    struct ggml_tensor * inp_cls = build_inp_cls();
                    cur = ggml_get_rows(ctx0, inp, inp_cls);
                } break;
```
And like `inp_mean` we have `inp_cls` tensor as a member of `llama_context`
```c++
struct llama_context {
    ...
    struct ggml_tensor * inp_cls;         // I32 [n_batch]
};
```
And in `append_pooling` we have:
```c++
        switch (pooling_type) {
            ...
            case LLAMA_POOLING_TYPE_CLS:
            case LLAMA_POOLING_TYPE_LAST:
                {
                    struct ggml_tensor * inp_cls = build_inp_cls();
                    cur = ggml_get_rows(ctx0, inp, inp_cls);
                } break;
            ...
```
And `build_inp_cls` looks like this:
```c++
    struct ggml_tensor * build_inp_cls() {
        lctx.inp_cls = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);
        cb(lctx.inp_cls, "inp_cls", -1);
        ggml_set_input(lctx.inp_cls);
        return lctx.inp_cls;
    }
```
In `llama_decode_internal` will then have the following:
```c++

    if (cparams.embeddings && cparams.pooling_type == LLAMA_POOLING_TYPE_LAST) {
        const int64_t n_tokens = batch.n_tokens;

        GGML_ASSERT(lctx.inp_cls);
        GGML_ASSERT(ggml_backend_buffer_is_host(lctx.inp_cls->buffer));

        uint32_t * data = (uint32_t *) lctx.inp_cls->data;
        memset(lctx.inp_cls->data, 0, n_tokens * ggml_element_size(lctx.inp_cls));

        std::vector<int> last_pos(n_tokens, -1);
        std::vector<int> last_row(n_tokens, -1);

        for (int i = 0; i < n_tokens; ++i) {
            const llama_seq_id seq_id = batch.seq_id[i][0];
            const llama_pos    pos    = batch.pos[i];

            GGML_ASSERT(seq_id < n_tokens && "seq_id cannot be larger than n_tokens with pooling_type == LAST");

            if (pos >= last_pos[seq_id]) {
                last_pos[seq_id] = pos;
                last_row[seq_id] = i;
            }
        }

        for (int i = 0; i < n_tokens; ++i) {
            if (last_row[i] >= 0) {
                data[i] = last_row[i];
            }
        }
    }
```
```console
(gdb) p *lctx.inp_cls
$4 = {type = GGML_TYPE_I32, backend = GGML_BACKEND_TYPE_CPU, buffer = 0x555555aab7c0, ne = {6, 1, 1, 1}, nb = {
    4, 24, 24, 24}, op = GGML_OP_NONE, op_params = {0 <repeats 16 times>}, flags = 1, grad = 0x0, src = {0x0, 
    0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0}, view_src = 0x0, view_offs = 0, data = 0x7ffe3b22e020, 
  name = "inp_cls", '\000' <repeats 56 times>, extra = 0x0}
```
After the first for loop we have:
```console
(gdb) p last_pos
$7 = std::vector of length 6, capacity 6 = {5, -1, -1, -1, -1, -1}
(gdb) p last_row
$6 = std::vector of length 6, capacity 6 = {5, -1, -1, -1, -1, -1}
```
We then loop over all the tokens in the sequence again and set the
`data/lctx.inp_cls` to the last row index for that sequence:
```c++
        for (int i = 0; i < n_tokens; ++i) {
            if (last_row[i] >= 0) {
                data[i] = last_row[i];
            }
        }
```
So for `i=0`, `last_row[0]` is 5 so we set `data[0]` to 5. The rest are -1.
```console
(gdb) p data[0]
$11 = 5

gdb) p ((int*)lctx.inp_cls.data)[0]
$16 = 5
```
Later in `embeddings.cpp` and the `batch_decode` function we have:
```c++
        // try to get sequence embeddings - supported only when pooling_type is not NONE
        const float * embd = llama_get_embeddings_seq(ctx, batch.seq_id[i][0]);
```

So where mean pooling calculated the average of each dimension/feature for all
tokens in a sequence, last pooling simple returnes the last tokens embeddings.
```
Token 1: [1.0, 2.0, 3.0, 4.0]
Token 2: [2.0, 3.0, 4.0, 5.0]
Token 3: [3.0, 4.0, 5.0, 6.0]

Mean pooling:
Dimension 1: (1.0 + 2.0 + 3.0) / 3 = 2.0
Dimension 2: (2.0 + 3.0 + 4.0) / 3 = 3.0
Dimension 3: (3.0 + 4.0 + 5.0) / 3 = 4.0
Dimension 4: (4.0 + 5.0 + 6.0) / 3 = 5.0

Result: [2.0, 3.0, 4.0, 5.0]

Last pooling:

Result: [3.0, 4.0, 5.0, 6.0]
```
I've been struggling to understand the usefulness of last pooling but I think I
need to keep in mind that this embedding is the result of the attention
mechanism so just because the last token in the sequence might "look" like it
represent a subword/word/sentence it might not be the case. For example, lets 
say we have the following input sequence that we want to created token embeddings
for: "I was skeptical at first, but in the end, I loved it!"
The last token embedding might represent the `!` which I did not think would be
useful but in this case the returned token embedding is a semantic
representation of this token in the context of the entire sequence.
The effectiveness of last pooling can vary depending on the specific
architecture of the model. Some models might be better at encoding full-sequence
information into the final token than others.

And then we have the `CLS` (Classification) pooling which is the same as last
pooling but it will only use the first token embedding in the sequence which is
the special `cls` token embedding.
This approach is inspired by models like BERT, which use a special [CLS] token
at the beginning of each sequence. The idea is that during training, the model
learns to encode sequence-level information into this first token.
This method assumes that the first token can adequately represent the entire
sequence, which may or may not be true depending on the model's architecture
and training.

### Bidirectional Attentions for Embeddings
With causal attention, the model can only attend to previous tokens in the
sequence. This is true even for the initial prompt which is passed to the
llm, but it is still only allowed to look at the previous tokens from the
current token being processed.
But for embeddings we want to be able to look at the entire sequence.

Causal attention:
```console
Token:  [A] [B] [C] [D]
A can see: A
B can see: A, B
C can see: A, B, C
D can see: A, B, C, D
```
Causal mask:
```console
[[1, 0, 0, 0],
 [1, 1, 0, 0],
 [1, 1, 1, 0],
 [1, 1, 1, 1]]
```

Bi-directional attention:
```console
Token:  [A] [B] [C] [D]
A can see: A, B, C, D
B can see: A, B, C, D
C can see: A, B, C, D
D can see: A, B, C, D
```
Bi-directional mask:
```console
[[1, 1, 1, 1],
 [1, 1, 1, 1],
 [1, 1, 1, 1],
 [1, 1, 1, 1]]
```

In the case of a non-causal model, one where there is no need for a kv-cache, the
attention mask is created in `llama-graph.cpp` and the function `llm_graph_input_attn_no_cache::set_input`:
```console
void llm_graph_input_attn_no_cache::set_input(const llama_ubatch * ubatch) {
    const int64_t n_kv     = ubatch->n_tokens;
    const int64_t n_tokens = ubatch->n_tokens;

    GGML_ASSERT(kq_mask);
    GGML_ASSERT(ggml_backend_buffer_is_host(kq_mask->buffer));

    float * data = (float *) kq_mask->data;

    for (int h = 0; h < 1; ++h) {
        for (int i1 = 0; i1 < n_tokens; ++i1) {   // iterate over all the tokenns in the micro batch
            const llama_seq_id s1 = ubatch->seq_id[i1][0]; // get the sequence of the id of the first token.

            for (int i0 = 0; i0 < n_tokens; ++i0) {
                float f = -INFINITY;

                for (int s = 0; s < ubatch->n_seq_id[i0]; ++s) {
                    const llama_seq_id s0 = ubatch->seq_id[i0][0]; // get the sequence id of the first token

                    // TODO: reimplement this like in llama_kv_cache_unified
                    // Only attend to tokens from the same sequence and if causual attention then a token
                    // can only attend to tokens that are before it in the sequence.
                    if (s0 == s1 && (!cparams.causal_attn || ubatch->pos[i0] <= ubatch->pos[i1])) {
                        if (hparams.use_alibi) {
                            f = -std::abs(ubatch->pos[i0] - ubatch->pos[i1]);
                        } else {
                            f = 0.0f;
                        }
                        break;
                    }
                }

                data[h*(n_kv*n_tokens) + i1*n_kv + i0] = f;
            }
        }
    }
}
```

One thing to keep in mind is that embeddings are not like causal attention. For casual attention
the model will store the computed Key and Value vectors for each token so that they don't have to
be recomputed for each token in the sequence. This is not the case for embeddings because embeddings
are processed in parallel in a single forward/decode pass. So embedding models don't need memory
which is what this is called now.
For example if we take a look in `llama_model.cpp` we can see that the following:
```c++
llama_memory_i * llama_model::create_memory(const llama_memory_params & params, llama_cparams & cparams) const {
    llama_memory_i * res;

    switch (arch) {
        // Models that need specific instantiation should be handled in the
        // switch statement
        case LLM_ARCH_BERT:
        case LLM_ARCH_JINA_BERT_V2:
        case LLM_ARCH_NOMIC_BERT:
        case LLM_ARCH_NOMIC_BERT_MOE:
        case LLM_ARCH_NEO_BERT:
        case LLM_ARCH_WAVTOKENIZER_DEC:
        case LLM_ARCH_DREAM:
            {
                res = nullptr;
            } break;
        ....
``` 
Notice that these are setting the `res` variable which is of type `llama_memory_i` to nullptr.
So let say we have a version of a model, which also has an embedding model, which will
probably have to be able to differentiate between the two so that the embedding model does
not have memory and the LLM model does.

### Bidirectional attentions with Sliding Window Attention
So the attention for an embedding model is full attention. But is it is possible
that a model uses sliding window attention. This would have an impact on thei
attention mask. We still need a bidirectional mask but the attention will only
be applied to a limited number of tokens in the sequence, the size of the window
in both directions.

For sliding window attention in the standard way, it will only look at the
previous tokens in the sequence. For bidirectional attention though, we want it
to also look forward in the sequence but not the entire sequence which would be
the case with full attention. For this we can introduce a symmetric sliding
window attention type (`LLAMA_SWA_TYPE_SYMMETRIC`).

For example, if we have a sequence with 7 tokens and a `n_swa` of 2, each token
can attend to itself plus 1 token in each direction (`half_window = n_swa/2 = 1`).

For example, if we have a sequence with 7 token and a `n_swa` of 2 the attention
mask would look like this:
```console
=== Attention mask ===
n_swa : 2, n_kv: 7, swq_type: symmetric
'0' = can attend, '∞' = masked
Rows = query tokens, Columns = key/value tokens

     0 1 2 3 4 5 6
 0:  0 0 ∞ ∞ ∞ ∞ ∞
 1:  0 0 0 ∞ ∞ ∞ ∞
 2:  ∞ 0 0 0 ∞ ∞ ∞
 3:  ∞ ∞ 0 0 0 ∞ ∞
 4:  ∞ ∞ ∞ 0 0 0 ∞
 5:  ∞ ∞ ∞ ∞ 0 0 0
 6:  ∞ ∞ ∞ ∞ ∞ 0 0
```

### Late Interaction Models
There are models like https://huggingface.co/LiquidAI/LFM2-ColBERT-350M which use
something called late interaction. How this works is that we use the embeddings
are we normally would in llama.cpp, for example:
```console
./build/bin/llama-embedding -m models/LFM2-ColBERT-350M.gguf -p "Hello world today" --pooling none --embd-normalize -1
...
batch_decode: n_tokens = 4, n_seq = 1

embedding 0:  3.956538  4.565637  0.665844  ... -1.282995  3.684453  3.013499 
embedding 1:  5.020997  3.513428 -0.382252  ... -0.476009  1.109715  3.843809 
embedding 2:  4.341805  2.081467  1.026007  ... -0.938215  3.985719  2.788224 
embedding 3:  4.500803  2.187851 -0.308470  ...  0.234649  2.836646  3.167321 
```
So this will generate a reduces embedding space, instead of the normal 1024 for
this model each token embedding is just 128 dimensions. This is handled by the
1_Dense layer of the sentence_transformer in the original model:
```console
$ cat ~/work/ai/models/LFM2-ColBERT-350M/1_Dense/config.json 
{"in_features": 1024, "out_features": 128, "bias": false, "activation_function": "torch.nn.modules.linear.Identity"}
```
This is a down projection which is performed after the transformer layers. This
tensor is converted into a gguf tensor as part of the model conversion and they
are then applied to the model's computation graph in llama-model.cpp:
```c++
    llm->build_dense_out(dense_2_out_layers, dense_3_out_layers);

    llm->res->set_outputs();

    return llm->res->get_gf();
}
```

Now, normally for embeddings we would apply some type of pooling and the output
we get would be just a single embedding vector for the entire input sequence.
This is easy to handle but for long documents we might loose important information
when compressing the entire document into a single vector. The ColBERT solution
is to keep all the token embeddings (one per token) but instead of storing 1024
floats per token it stores only 128.

And normally we would also compare a query embedding against the document
embedding to figure out how similar they are. Instead we have multiple tokens
and we compare them like this:
```console
Query   : "Hello world"                   (2 tokens -> 2 vectors: Q_1, Q_2)
Document: "Greetings to the entire world" (5 tokens -> 5 vectors: D_1...D_5)

1. Take Q_1 and compare it to all document vectors D_1...D_5
   Does "Hello" match "Greetings"? (Score: 0.8)
   Does "Hello" match "world"? (Score: 0.1)
   ...
   Keep the Max: The best match for "Hello" is "Greetings" (0.8).

2. Take Q_2 and compare it to all document vectors D_1...D_5
   Does "world" match "Greetings"? (Score: 0.2)
   Does "world" match "world"? (Score: 0.9)
   ...
   Keep the Max: The best match for "world" is "world" (0.9).

3. Sum the Maxes: Total Score = $0.8 + 1.0 = 1.8$.
```
This algorithm is called Maximum Similarity (MaxSim).

In practice the vector embeddings are stored something like this:
```console
Document Id     Original text           Token Embeddings
doc_42      	"Hello world today..."	[[0.1, ...], [0.5, ...], [0.2, ...]]
doc_43	        "The weather is..."	    [[0.9, ...], [0.1, ...], [0.8, ...]]
```
So when we want to find documents similar to a query we first generate the query
embedding. We then compare each token a specific document id row above and get
the maxsim score for that document.We repeat this for all documents in the
database/index and then return the top N documents with the highest scores.
