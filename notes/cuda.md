## CUDA (Compute Unified Device Architecture)
The motivation for this page is that I want to get a basic understanding of
how GPUs work from a programming perspective. I can understand that there are
libraries like Torch and TensorFlow that can do this for us but is just seems
like magic to me at the moment.

CUDA is a parallel computing platform and programming model developed by Nvidia
for general computing on its own GPUs (graphics processing units).

### CUDA Ecosystem
```
Dev tools:     NVIDIA SMI  Data Center GPU Mgr  GPU REST Engine

Libraries:     cuBlas   cuFFT  cuSPARSE  cuSOLVER  AGM-X
               Thrust   CUB    cuDNN     cuRand    NCCL

Compilers:     nvcc,nvc  CUDA-GDB  NVIDIA Nsight NVIDIA Visual Profiler PAPI CUDA
               nvc++
               nvfortran

Programming:   CUDA       OpenMP API   OpenACC   OpenCL   PyCUDA
models

Drivers:       Linux and Windows device drivers and runtime (no mac?)
```

`CUB` (CUDA UnBound) is a library of high-performance primitives for CUDA.
`AMG-X` (Adaptive General Matrix eXponentiation)
`NCCL` (NVIDIA Collective Communications Library) is a library that provides
`multi-GPU` and `multi-node` collective communication primitives.

### Compilation units
A CUDA program would have a `.cu` suffix. The nvcc compiler will separate out
the host (CPU) code from the device (GPU) code. The host code is compiled by
the host compiler (gcc, clang, etc.) and the device code is compiled by the
NVIDIAS proprietary backend compiler named PTXAS into an intermediate
representation (PTX) which is later converted into binary code for the GPU.

The host will call code on the GPU using something that is called kernel calls.
A kernel is a function that is executed on the GPU. Each kernel functions runs
the same code but on different data.

```c++
__global__ void add_arrays(int *a, int *b, int *c, int size) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < size) {
        c[tid] = a[tid] + b[tid];
    }
}
```
The `__global__` keyword is used to indicate that this function is a kernel
function. The `add_arrays` function will be executed on the GPU. The `tid`
variable is the thread id. The `blockIdx.x` is the block id and the
`threadIdx.x` is the thread id within the block. The `blockDim.x` is the
number of threads in a block. The `blockIdx.x * blockDim.x + threadIdx.x`
is the global thread id.

There is an example in [array_add](../gpu/cuda/src/array_add.cu) that shows the
above example.

### Parallel Thread Execution (PTX)
When you compile CUDA code with nvcc, the device code doesn't immediately get
translated to machine code. Instead, it first gets compiled to this intermediate
PTX format. The abstraction allows CUDA code to be compiled and then later
be translated into the binary code of a specific GPU.

```console
$ cd gpu/cuda
$ make array-add-ptx
nvcc -ptx src/array-add.cu
//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_52
.address_size 64

	// .globl	_Z10add_arraysPiS_S_i
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 1 .b8 $str[52] = {98, 108, 111, 99, 107, 73, 100, 120, 46, 120, 32, 61, 32, 37, 100, 44, 32, 98, 108, 111, 99, 107, 68, 105, 109, 46, 120, 32, 61, 32, 37, 100, 44, 32, 116, 104, 114, 101, 97, 100, 73, 100, 120, 46, 120, 32, 61, 32, 37, 100, 10};
.global .align 1 .b8 $str$1[45] = {91, 71, 80, 85, 93, 32, 97, 114, 114, 97, 121, 32, 105, 110, 100, 101, 120, 32, 91, 37, 100, 93, 58, 32, 97, 100, 100, 105, 110, 103, 32, 37, 100, 32, 43, 32, 37, 100, 32, 61, 32, 37, 100, 10};

.visible .entry _Z10add_arraysPiS_S_i(
	.param .u64 _Z10add_arraysPiS_S_i_param_0,
	.param .u64 _Z10add_arraysPiS_S_i_param_1,
	.param .u64 _Z10add_arraysPiS_S_i_param_2,
	.param .u32 _Z10add_arraysPiS_S_i_param_3
)
{
	.local .align 16 .b8 	__local_depot0[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<18>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd2, [_Z10add_arraysPiS_S_i_param_0];
	ld.param.u64 	%rd3, [_Z10add_arraysPiS_S_i_param_1];
	ld.param.u64 	%rd4, [_Z10add_arraysPiS_S_i_param_2];
	ld.param.u32 	%r2, [_Z10add_arraysPiS_S_i_param_3];
	add.u64 	%rd5, %SP, 0;
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	st.local.v2.u32 	[%rd1], {%r4, %r3};
	mov.u32 	%r5, %tid.x;
	st.local.u32 	[%rd1+8], %r5;
	mov.u64 	%rd6, $str;
	cvta.global.u64 	%rd7, %rd6;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd7;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r6, [retval0+0];
	} // callseq 0
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.s32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	cvta.to.global.u64 	%rd11, %rd3;
	add.s64 	%rd12, %rd11, %rd9;
	ld.global.u32 	%r7, [%rd12];
	ld.global.u32 	%r8, [%rd10];
	cvta.to.global.u64 	%rd13, %rd4;
	add.s64 	%rd14, %rd13, %rd9;
	add.s32 	%r9, %r7, %r8;
	st.global.u32 	[%rd14], %r9;
	ld.global.u32 	%r10, [%rd12];
	ld.global.u32 	%r11, [%rd10];
	st.local.v4.u32 	[%rd1], {%r1, %r11, %r10, %r9};
	mov.u64 	%rd15, $str$1;
	cvta.global.u64 	%rd16, %rd15;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd16;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b32 retval0;
	call.uni (retval0),
	vprintf,
	(
	param0,
	param1
	);
	ld.param.b32 	%r12, [retval0+0];
	} // callseq 1

$L__BB0_2:
	ret;

}
```

### CUBIN (CUDA Binary)
Is an ELF-formatted file. This contains CUDA executable code sections and
sections containing symbols, relocators, debug info. nvcc embeds cubin files
into the host executable file.

### nvcc
nvcc is the CUDA compiler driver (think gcc or clang). It is used to compile
CUDA programs. nvcc accepts a range of conventional compiler options, such as
for defining macros and include/library paths, and for steering the compilation
process. nvcc also accepts a range of CUDA-specific options for defining the
virtual architecture targeting which the CUDA program is compiled, and for
defining the memory model used, and for steering the compilation process.

### Questions
So when we are going to train a large language model on a GPU we first need to
load the model weights into the host's memory, then copy them over to the GPU's
memory, and then call a kernel function to execute. Is this how it is done in
practice or are there ways to avoid the memory copying?

When training large language models (or any deep learning models) on GPUs, the
model's weights and the data do need to reside in the GPU's memory. However, the
process is a bit more nuanced than just loading everything into the host's
memory and then copying it to the GPU's memory. Here's a breakdown of how it
typically works in practice:

Model Initialization:

When you initialize a model using deep learning frameworks like TensorFlow or
PyTorch, and you've set the device to a GPU, the model's weights are often
directly initialized in the GPU's memory. There's no need to first initialize
them on the CPU and then transfer them.
When you instruct these frameworks to initialize tensors (or model parameters)
on the GPU, a series of steps occur:

* The framework communicates with the GPU through a driver API (e.g., CUDA for
NVIDIA GPUs).
* Memory on the GPU is allocated to store the tensor.
* Initialization operations (like random number generation for weight
initialization) are executed as GPU kernels. These operations fill the allocated
memory with the initial values.

The key takeaway is that these operations occur directly on the GPU without the
need for an intermediary step on the CPU.


Data Loading and Batching:


Training data is usually read in batches. Instead of loading the entire dataset
into the host's memory and then transferring it to the GPU, data is typically
loaded batch-by-batch. Each batch is transferred to the GPU just before it's
needed for training.
Modern deep learning frameworks and data loaders handle this process
efficiently, often using asynchronous operations to overlap data loading on the
CPU with computation on the GPU.

Once the model's weights are on the GPU, they typically stay there throughout
the training process. Forward passes, backward passes, and weight updates all
happen on the GPU. The weights aren't constantly moved back and forth between
the host and the GPU.
It's only if you need to save the model's weights or inspect them on the CPU
that you'd transfer them back to the host's memory.

### Streams
A stream in CUDA is a sequence of commands that execute in order. It is like a
queue of commands that are executed one after the other. Now, what I've been 
doing in my examples is just using synchronous commands, like `cudaMemcpy` and
and not specifying a stream when calling a kernel. When we call a function
like `cudaMemcpy` this is a synchronous operation, the host program will not
progress (it will block) until the entire memory transfer is complete.
When we call `cudaMemcpy` this function is added to the default CUDA stream. The
default stream operations are all executed in the order they are called/added.
In this way `cudaMemcpy` acts as a synchronization point between the host and
the device.

When using streams we use the async version of memcpy, like `cudaMemcpyAsync`,
and for launching a kernel we specify a stream as an argument.

An example can be found in [streams.cu](../gpu/cuda/src/streams.cu).

### Copy engine
Some (perhaps all I'm not sure) NVidia GPUs have a copy engine. This is a
dedicated hardware unit that can copy data between the host and the device
without involving the GPU's compute units. This can be used in combination with
streams to overlap data transfers with computation. Most have a host-to-device
copy engine and a device-to-host copy engine.


###  CMAKE_CUDA_FLAGS
This is a cmake variable that is passed to `nvcc` when compiling CUDA code.

#### FASTFP16_AVAILABLE
This flag tells nvcc to enable the FP16 code paths in the CUDA code.
```console
$ cmake -S . -B build -DGGML_CUDA=ON -DCMAKE_CUDA_FLAGS="-DFASTFP16_AVAILABLE"
```

### Compute Compability (CC)
https://developer.nvidia.com/cuda-gpus

I have have a `GeForce RTX 4070` which has a compute capability of `8.9`.
```console
$ ./minimal
CUDA Runtime version: 12.6
CUDA Driver version: 12.6
CUDA device count: 1
Device 0 - Total VRAM: 11.62 GB

Device 0:
  Name: NVIDIA GeForce RTX 4070
  Compute Capability: 8.9
  Multiprocessors: 46
  Clock Rate: 2505 MHz
  Total Global Memory: 11.62 GB
  L2 Cache Size: 36.00 MB
CUDA program ran successfully
```

CC 8.0, 8.6: Ampere (RTX 30 series, A100)
CC 8.9:      Ada Lovelace (RTX 40 series)
CC 9.0:      Hopper (H100)

The compute capability is something that can be set on the command line as the
`-arch` flag. For example:
```console
$ nvcc -arch=sm_89 -o $@ $<
```
Where `sm` stands for streaming multiprocessor and `89` is the compute
capability. Using this tells the compiler to generate code that is optimized
these specific GPUs.

### NPPC (NVIDIA Performance Primitives Core)
This is a collection of GPU-accelerated image, video, and signal processing
functions.

### Threads/memory
If we think about how memory cells for we have small
[capacitators](https://github.com/danbev/learning-iot?tab=readme-ov-file#capacitors)
that can store a 0 or a 1. These cells are arranged in a grid and can be read
using an row address and a column addres. The row address part of the request
address to read will activate a row in this grid. This will cause the cells in
the row to be connected to the column lines which will then be available to be
read from the sense amplifiers.
```
Address:   00110001001001111010001101101110011
          |<--- ROW ADDR --->|<-- COL ADDR -->|
               │                    │
               │                    │
               ▼                    ▼
    ┌──────────────┐         ┌────────────────┐
    │              │         │                │
    │ ROW DECODER  │         │ COLUMN DECODER │
    │              │         │                │
    └──────┬───────┘         └───────┬────────┘
           │                         │
           │                         │
           │                         │
           │                         ▼
           │            ┌──────────────────────────────┐
           │            │      SENSE AMPLIFIERS        │
           │            │  ▲   ▲   ▲   ▲   ▲   ▲   ▲   │────► DATA
           │            │  │   │   │   │   │   │   │   │     BUFFERS
           │            └──┼───┼───┼───┼───┼───┼───┼───┘
           │       |   |   │   │   │   │   │   │   │
           │       |   |   │   │   │   │   │   │   │
           │       |   |   │   │   │   │   │   │   │
           │       |   |   │   │   │   │   │   │   │
           │       |   |   │   │   │   │   │   │   │
           │    Selected   │   │   │   │   │   │   │
           │    Row|   |   │   │   │   │   │   │   │
           ▼       |   |   │   │   │   │   │   │   │
    ┌─────────────►┌───┬───┬───┬───┬───┬───┬───┬───┐
    │              │ C │ C │ C │ C │ C │ C │ C │ C │  Activated Row
    │              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘  (Word Line)
    │                │   │   │   │   │   │   │   │
    │              ┌─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┐
    │              │ C │ C │ C │ C │ C │ C │ C │ C │
    │              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘
    │              ┌─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┐
    │              │ C │ C │ C │ C │ C │ C │ C │ C │
    │              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘
    │              ┌─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┐
    │              │ C │ C │ C │ C │ C │ C │ C │ C │
    │              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘
    │              ┌─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┐
    │              │ C │ C │ C │ C │ C │ C │ C │ C │
    │              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘
    |              ┌─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┐
    |              │ C │ C │ C │ C │ C │ C │ C │ C │
    |              └─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┴─┬─┘
    │
    │               Bit Lines
    │              (connecting cells to sense amplifiers)
    └──────────────────────────────────────────────────┘
                          MEMORY ARRAY

 ┌───┐
 │ 1 │ READ OPERATION:
 └───┘
     1. Row address is decoded by row decoder
     2. Selected word line activates ALL cells in that row
     3. ALL cells in row connect to bit lines
     4. Data from entire row flows UP to sense amplifiers
     5. Column decoder selects which bit from sense amplifiers to read
     6. Reading drains ALL capacitors in the row (destructive read)
     7. Data must be written back to entire row after reading
```
When a cell is read it is drained, this charge (or lack of charge) creates only
a very small voltage difference on the bit line (typically just tens of
millivolts). The sense amplifier detects this extremely weak signal. Once
detected, the sense amplifier boosts this tiny voltage difference to full logic
levels (0V for a logic "0" and typically 1.1-3.3V for a logic "1").

Reading a memory cell is a destructive operation. The act of reading the cell
drains the charge from the cell, so the data must be written back to the cell
after reading. This is done by sense amplifiers, the refresh the memory cells
which makes sense since they have the data that was read. This is called 
write back. So when a new row need to be read the old row has be be written back
first.

One thing to keep in mind is that a row and a page often means the same thing,
so if you hear the term page in the context of memory it is often referring to
a row in the memory array. So reading a page is the same thing as reading a row.

Now the column address is actually just accessing a single column/bitline/memory
cell, and this is read from the strong voltage in the sense amplifier. Now, we
can read repeatably from the sense amplifier and note that all cells in the row
are actually available, not just the column that we want to read. This is where
the one advantage of storing data in consecutive memory cells is an advantage.
They can be read without having to got through the row selection process.

So lets say that a the time to read a new column is 16 cycles, and the time
to read a new row/page is also 16 cycles, as well as the time to write back
a row/page is 16 cycles.
Then to read a new page we have to read the row, read the column and write it
back which is 16 + 16 + 16 = 48 cycles. If we want to read the next column
we only have to read the column which is 16 cycles. So reading the next column
is faster than reading the next page.

This is why data read patterns are important. If we read data in a sequential
pattern we can take advantage of the fact that the data is already in the sense
amplifiers and we can read it faster.


So this only depends on how I access the memory of my input for example. Like
there is nothing that CUDA does to "introspect" my code or anything, it is only
the access patterns that I use that determines if my code will be efficient
regarding memory. If I access memory in consecutively the memory subsystem
(like we discussed above) will work efficiently, but if I read data in patterns
that "jump" around and access different rows things will not be as performant.

To tie this back to CUDA, lets take a look a the following kernel function:
```c++
__global__ void add_arrays(int* a, int* b, int* c, int size) {
    // Calculate the index of array index that this thread will process.
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        c[idx] = a[idx] + b[idx];
        printf("[GPU] array index [%d]: adding %d + %d = %d\n", idx, a[idx], b[idx], c[idx]);
    }
}
```
Say we have 2 thread blocks and 4 threads in each block:
```
block_0: thread_0, thread_1, thread_2, thread_3
block_1: thread_0, thread_1, thread_2, thread_3
```
Now, the variable `idx` will be the following for these threads:
```
Block 0:
Thread 0: 0 * 4 + 0 = 0
Thread 1: 0 * 4 + 1 = 1
Thread 2: 0 * 4 + 2 = 2
Thread 3: 0 * 4 + 3 = 3

Block 1:
Thread 0: 1 * 4 + 0 = 4
Thread 1: 1 * 4 + 1 = 5
Thread 2: 1 * 4 + 2 = 6
Thread 3: 1 * 4 + 3 = 7
```
So when the function runs the accesses will be like this:
```
Block 0, Warp 0 (contains threads 0-3):
Thread 0: a[0], b[0], c[0]
Thread 1: a[1], b[1], c[1]
Thread 2: a[2], b[2], c[2]
Thread 3: a[3], b[3], c[3]

Block 1, Warp 0 (contains threads 4-7):
Thread 0: a[4], b[4], c[4]
Thread 1: a[5], b[5], c[5]
Thread 2: a[6], b[6], c[6]
Thread 3: a[7], b[7], c[7]
```
So when these threads execute they are accessing memory in a consecutive pattern
so the GPU memory subsystem can read a single DRAM row and read many values in
on memory transaction.

There's no special "magic" or introspection that CUDA brings to memory access
patterns. The memory subsystem in GPUs works on the same fundamental DRAM
principles we discussed earlier - with rows, columns, sense amplifiers, and the
efficiency advantage of accessing consecutive locations within the same row.

In CUDA we have a grid or work, which gets split into many blocks, and there
are many threads in each block.

Thread block:
```
  +-----------------------------------+
  | | | | | | | | | | | | | | | | | | |
  | | | | | | | | | | | | | | | | | | |
  | | | | | | | | | | | | | | | | | | |
  | | | | | | | | | | | | | | | | | | |
  | ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ |
  +-----------------------------------+
```
A block has a fixed number of threads and these are guarenteed to run
simultaneously on the same SM.

Each thread as it's own registers and can maintain it's own state. But the
program counter is not per thread but instead per group of 32 threads called
a warp.

```c++
__global__ void add_arrays(int* a, int* b, int* c, int size) {
    // Calculate the index of array index that this thread will process.
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        if (idx == 4) {
            printf("Thread idx: %d\n", idx);
            // skip this compuation.
        }
        c[idx] = a[idx] + b[idx];
        printf("[GPU] array index [%d]: adding %d + %d = %d\n", idx, a[idx], b[idx], c[idx]);
    }
}
```
What will happen is that there will be a hardware activity mask which will
mask all threads but thread 4 as active. There state will be maintained but
the won't actually execute. When a warp executes a path that only some threads
need to take, the execution units for the inactive threads sit idle. In our
example, while executing the if (idx == 4) block, 31 out of 32 execution units
(97%) are doing nothing.

The hardware masking is specific to the GPU architecture. Each warp has a 32 bit
mask (one for each thread) where a 1 means that the thread is active and a 0
means that the thread is inactive. When a conditional statement is encountered
in a kernel, the hardware evaluates the condition for each thread in the warp
and sets the mask accordingly. This is sometimes called a predicate mask as it
is a predicate for the execution of the thread.

So how does the code I write: if (idx == 4) set the predicate mask? Is there an
instruction that is added to the intermediate representation and then compiled
into the resulting binary?

The compiler, nvcc will add this to the generated PTX code. If we inspect
the PTX code we can see this in action:
```console
$ make array-add-ptx 
nvcc -ptx src/array-add.cu
//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35059454
// Cuda compilation tools, release 12.6, V12.6.85
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_52
.address_size 64

	// .globl	_Z10add_arraysPiS_S_i
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
...
.visible .entry _Z10add_arraysPiS_S_i(
	.param .u64 _Z10add_arraysPiS_S_i_param_0,
	.param .u64 _Z10add_arraysPiS_S_i_param_1,
	.param .u64 _Z10add_arraysPiS_S_i_param_2,
	.param .u32 _Z10add_arraysPiS_S_i_param_3
)
{
    ...
    // This is what calculates the idx variable. MAD = multiple and add
    // lo = use lower bits of the result
    // %r1 = destination register
    // %r4, %r3, %r5 = source registers
    mad.lo.s32 	%r1, %r4, %r3, %r5;

    // First predicate, idx < size
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB0_4;

    // Second predicate, if (idx == 4)
	setp.ne.s32 	%p2, %r1, 4;
	@%p2 bra 	$L__BB0_3;
```
`setp` is the set predicate.
setp.ne.s32 %p2, %r1, 4; compares if idx != 4
The predicate register %p2 is set to true if idx ≠ 4
@%p2 bra $L__BB0_3; means "if %p2 is true, branch to label L__BB0_3"
This is negated logic - "if idx is NOT 4, skip the special case"
L__BB0_3 is the label for the code after your if-block

So this is how the masking works.

32 threads per warp and we have 4 of them active at once this gives 128 threads
per excecution. If we access 8 bytes for this gives 128 * 8 = 1024 bytes which
is the row/page size. So this means that each time we execute one row/page will
be accessed which is very efficient.

Thread Organization:
* 32 threads per warp (standard CUDA warp size)
* 4 warps active concurrently (typical for one SM/streaming multiprocessor)
* Total of 128 threads executing in parallel

Memory Access Pattern:
* Each thread accesses 8 bytes of data (e.g., a double or two integers)
* 128 threads × 8 bytes = 1024 bytes (1 KB) total accessed


DRAM Organization:
* DRAM page/row size is 1 KB
* This means all 128 threads' memory accesses fit perfectly in one DRAM row

The efficiency comes from this alignment: when all these threads execute
together, they collectively access exactly one full DRAM row. This is maximally
efficient because:

* Row Activation Efficiency: A single row activation serves all 128 threads
* Memory Throughput: You get full utilization of the activated row
* No Row Conflicts: All threads work within the same row, avoiding the penalty
  of switching between rows

### Shared Memory
Before trying to understand shared memory and how it is useful I think it
might help to understand a basic matrix multiplication example.

```
                      B0 B1 B2 B3
    +--+--+--+--+    +--+--+--+--+    +-----+-----+-----+-----+
A0  |  |  |  |  |    |  |  |  |  |    |A0/B0|A0/B1|A0/B2|A0/B3|
    +--+--+--+--+    +--+--+--+--+    +-----+-----+-----+-----+
A1  |  |  |  |  | x  |  |  |  |  | =  |A1/B0|A1/B1|A1/B2|A1/B3|
    +--+--+--+--+    +--+--+--+--+    +-----+-----+-----+-----+
A2  |  |  |  |  |    |  |  |  |  |    |A2/B0|A2/B1|A2/B2|A2/B3|
    +--+--+--+--+    +--+--+--+--+    +-----+-----+-----+-----+
A3  |  |  |  |  |    |  |  |  |  |    |A3/B0|A3/B1|A3/B2|A3/B3|
    +--+--+--+--+    +--+--+--+--+    +-----+-----+-----+-----+
```
Now, each of the elements in the output matrix are independent of each other
and can be calculated in parallel which is great. But we must also consider
that while C(0,0) uses the A0 row, C(0,1) does as well.
Imaging we have 16 threads to perform this, each thread calculating one element
of the output matrix C:
```
Thread 0: Read row A0 from global memory, read column B0 from global memory, calculate C(0,0)
Thread 1: Read row A0 from global memory, read column B1 from global memory, calculate C(0,1)
...
```
This is not very efficient since we are reading the same row from global memory
multiple times. And as we discussed previously reading from global memory is
expensive. This is where shared memory comes in. So instead of reading from
global memory multiple times we only want to read one and store the data in
shared memory, which is memory that is on the SM itself. 
For example, we can define a shared memory matrix like this:
```c++
    __shared__ float sharedA[4][4];
```
Then each thread can read one value from global memory, the input matrix A in
this case and store that value in shared memory:
```c++
   sharedA[threadIdx.y][threadIdx.x] = A[row * width + threadIdx.x];
```
Keep in mind that we most likely have 16 thread running this kernel at the same
time so these will be read in parallel.
We would do the same thing for matrix B as well, the other input matrix.

We need to synchronize the threads after this so that all threads have written:
```c++
    __syncthreads();
```
After that we can perform the matrix multiplication using the shared memory:
```c++
    if (row < width && col < width) {
        float sum = 0.0f;
        
        // Compute the dot product using shared memory
        for (int k = 0; k < width; k++) {
            sum += sharedA[threadIdx.y][k] * sharedB[k][threadIdx.x];
        }
        
        C[row * width + col] = sum;
    }
```
