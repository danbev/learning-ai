## Embedding Vector
When working with natural language models the input is text but neural networks
operate on matrices/tensors of numbers, not text. Embedding vectors are a way
to turn text into vectors with the elements of the vector being numbers so that
text can be used with neural networks.

How this text to vectors of numbers is done can differ, and one example is doing
[one-hot-encoding](./one-hot-encoding.md). Another option is using a count
based approach. And we also have the option to use embeddings which is what this
document will address.

It's a way of representing data like strings, music, video as points in an
n-dimension space. Doing this can allow similar data points to cluster together.

Word to vector (Word2Vec) was invented by Google in 2013 which takes as input
a word and outputs an n-dimensional coordinate, a vector. So, simliar words
would be closer to each other. Think of this as the tip of the vectors are in
close proximity to each other if the words are simliar.

For songs similar sounding songs would be nearby each other. And for images,
simliar looking images would be closer to each other. This could be anything
really so we don't have to think just in terms of words.

How is this useful?  
Well, we can look at vectors/points close to get similar words, songs, movies
etc. This is called nearest neighbor search.

Embedding also allows us to compute similarity scores between these points
allowing us to ask how similar is this song to another song. We can use the
Euclidean distance, the dot product, cosine distance, etc to calculate this
distance.

The learning stage is what positions these vectors close to each other.

We can use a one dimensional vector to represent each word, for example:
```
Why He She They Gras Tree
1   3   4   5    10   11

        She                  Tree
Why   He   they           Gras
|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|-
1  2  3  4  5  6  7  8  0 10 11 12 13 14 
```
This way we can calculate how similar to word are using:
```
Why and Gras:
10 - 1 = 9

He and she:
4 - 3 = 1
```
We can also define the encoding should have two values per vector making it
2 dimensional. So instead of each word being a single value it each one will
be a vector. We can use more dimensions as well and they can be.


### Universal sentence encoder
In this case entire sentences can be transformed into vectors which enables us
to do the same with sentences that we could with single words above.

### Tokens vs Embeddings
(This section was generated by ChatGPT-4)

Tokens are the basic units of text that a language model processes. In natural
language processing (NLP), tokenization is the process of converting input text
into a list of tokens. These tokens can be words, parts of words (like subwords
or syllables), or even characters, depending on the granularity of the model's
vocabulary. For example, the sentence "Hello, world!" might be tokenized into
["Hello", ",", "world", "!"].

Embeddings are dense vector representations of tokens. They are used to capture
the semantic meaning of tokens in a continuous vector space. Each token is
mapped to a vector of fixed size (the embedding size). These vectors are learned
during the training process of the language model. Embeddings are designed to
represent linguistic features of tokens so that tokens with similar meanings
have similar embeddings.

Here's how they differ and relate to each other:

Granularity: Tokens are discrete and correspond directly to elements of the
text. Embeddings are continuous and represent the abstract features of tokens.

Purpose: Tokens are used to chop up the text into processable pieces. Embeddings
are used to understand and process the semantic relationships between these pieces.

Representation: Tokens are often represented by integers or strings in a lookup
table. Embeddings are multi-dimensional floating-point vectors.

Usage in Models: Tokens are the input to the embedding layer of a language
model. The embedding layer then translates each token into its corresponding
embedding.

In the workflow of a language model like GPT, the process starts with raw text,
which is tokenized into tokens. These tokens are then passed through an
embedding layer to get their embeddings. The embeddings are what the model
actually processes to understand the text and generate responses or predictions.
